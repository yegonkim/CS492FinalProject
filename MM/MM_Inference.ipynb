{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MM_Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcBXy21VM85X"
      },
      "outputs": [],
      "source": [
        "# WAV2VEC pretrained\n",
        "!gdown https://drive.google.com/uc?id=14EE8yxx4q9TKL6dDNfsVWlGW-S0-tNnx\n",
        "# BERT pretrained\n",
        "!gdown https://drive.google.com/uc?id=1-4owby1oHeRtwfQC9mBwQWVZx5L2HHek\n",
        "\n",
        "!git clone https://github.com/yegonkim/CS492FinalProject.git  \n",
        "\n",
        "!mv bert-best.pth /content/CS492FinalProject/TER\n",
        "!mv wav2vec2-best.pth /content/CS492FinalProject/SER\n",
        "\n",
        "!pip install transformers\n",
        "!pip install vosk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GET VOSK\n",
        "!wget https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip\n",
        "!unzip -q -o vosk-model-en-us-0.22.zip"
      ],
      "metadata": {
        "id": "nvUqKIUvOkI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST A SAMPLE AUDIO FILE FROM OUR OWN DATASET\n",
        "!unzip -q -o /content/CS492FinalProject/dataset_A.zip\n",
        "\n",
        "# GET THE MM PRETRAINED MODEL\n",
        "!gdown https://drive.google.com/uc?id=1yYqsuzUZmJbToy7pOqZTxBOKLzRixDdP"
      ],
      "metadata": {
        "id": "W0EyNiXJQuF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.insert(1, '/content/CS492FinalProject/MM/')\n",
        "\n",
        "from CS492FinalProject.MM import inference\n",
        "\n",
        "inference.inference(\"/content/dataset_A/anger_hb_2.wav\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2NFtbZBOLKN",
        "outputId": "c0245f7b-0ac6-451e-ca65-0f187ea06c1a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Text: [i really wanted to hit him in the face]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'emotion': 'anger'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}